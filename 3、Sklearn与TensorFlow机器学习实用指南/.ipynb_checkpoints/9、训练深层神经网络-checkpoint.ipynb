{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练深层神经网络\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度消失/爆炸问题\n",
    "\n",
    "反向传播算法的工作原理是从输出层到输入层，传播误差的梯度。 一旦该算法已经计算了网络中每个参数的损失函数的梯度，它就使用这些梯度来用梯度下降步骤来更新每个参数。\n",
    "\n",
    "不幸的是，梯度往往变得越来越小，随着算法进展到较低层。\t结果，梯度下降更新使得低层连接权重实际上保持不变，并且训练永远不会收敛到良好的解决方案。这被称为 __梯度消失问题__。\t\n",
    "\n",
    "在某些情况下，可能会发生相反的情况：梯度可能变得越来越大，许多层得到了非常大 的权重更新，算法发散。这是 __梯度爆炸的问题__。\n",
    "\n",
    "###  减缓这一问题的方法一：Xavier初始化和He初始化\n",
    "\n",
    "提出需要保持每一层的输入和输出的方差一致，并且需要在反向流动过某一层时，前后的方差也要一致。\n",
    "\n",
    "当输入连接的数量大致等于输出连接的数量时，可以得到更简单的等式：\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "该方法的折中方案公式为Xavier初始化。\n",
    "\n",
    "ReLU激活函数的初始化方法有时称为He初始化。\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "venv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
