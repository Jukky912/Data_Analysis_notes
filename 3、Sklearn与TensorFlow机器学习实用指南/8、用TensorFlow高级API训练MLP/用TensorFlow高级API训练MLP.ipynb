{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\APYPLA~1\\venv_dir\\venv1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\APYPLA~1\\venv_dir\\venv1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\APYPLA~1\\venv_dir\\venv1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\APYPLA~1\\venv_dir\\venv1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\APYPLA~1\\venv_dir\\venv1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\APYPLA~1\\venv_dir\\venv1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "D:\\python\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.datasets import fetch_mldata\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_data = fetch_mldata('MNIST original',data_home='./')\n",
    "\n",
    "X,y= m_data['data'],m_data['target']\n",
    "x_train,x_test,y_train,y_test = X[:60000],X[60000:],y[:60000],y[60000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  因为有的算法可能会受到训练样例的顺序的影响，当在一行中得到许多相似的样例，算法的效果会很差，所以随机打乱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = np.random.permutation(60000)\n",
    "\n",
    "x_train,y_train = x_train[random_index],y_train[random_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用\tTensorFlow\t高级\tAPI\t训练\tMLP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与\t__TensorFlow__\t一起训练\t__MLP__\t最简单的方法是使用高级\t__API\tTF.Learn__ ，这与\t__sklearn__\t的\t__API__ 非常相似。\t__DNNClassifier__\t可以很容易训练具有任意数量隐层的深度神经网络，而\t__softmax__\t输出层输出估计的类概率。\n",
    "\n",
    "例如，下面的代码训练 __两个隐藏层__ 的\t__DNN__ （一个具有\t__300__\t个神经元，另一个具有\t__100__\t个神经元）和一个具有\t__10__\t个神经元的\t__SOFTMax__\t输出层进行分类：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_columns = tf.contrib.learn.infer_real_valued_columns_from_input(x_train)\n",
    "# dnn_clf = tf.contrib.learn.DNNClassifier(hidden_units=[300,100],n_classes=10,feature_columns=feature_columns)\n",
    "# dnn_clf.fit(x=x_train,y=y_train,batch_size=50,steps=40000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DNNClassifier__\t基于\t__Relu__\t激活函数创建所有神经元层（我们可以通过设置超参 数\t__activation_fn__\t来改变激活函数）。输出层基于\t__SoftMax__\t函数，__损失函数__ 是 __交叉熵__。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用普通\tTensorFlow\t训练\tDNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一、构造阶段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置输出特征形状和输出特征形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32,shape=(None,n_inputs),name='X')\n",
    "y = tf.placeholder(tf.int64,shape=(None),name='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建一个图层。它将需要参数来指定输入， 神经元数量，激活函数和图层的名称:\n",
    "\n",
    "\n",
    "#### 第三行后三行：\n",
    "\n",
    "创建一个保存权重矩阵的\tW\t变量。\t它将是包含每个输入和每个神经元之间 的所有连接权重的2D张量；因此，它的形状将是\t(n_inputs,\tn_neurons)\t。\n",
    "\n",
    "它将被随机初始化，__使用具有标准差为\t2/√n\t的截断的正态（高斯）分布__ (使用 __截断的正态分布而不是常规正态分布确保不会有任何大的权重__ ，这可能会减慢训练。), __使用这个特定的标准差有助于算法的收敛速度更快__。\n",
    "\n",
    "\n",
    "\n",
    "#### tf.truncated_normal 的用法：https://github.com/OneStepAndTwoSteps/TensorFlow_notes\n",
    "\n",
    "#### 截断的正态分布 https://www.zhihu.com/question/49923924"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def neuron_layer(X,n_neurous,name,activation=None):\n",
    "#     with tf.name_scope(name):\n",
    "#         n_inputs =int(X.get_shape()[1])\n",
    "        \n",
    "#         stddev = 2 /np.sqrt(n_inputs)\n",
    "#         init = tf.truncated_normal(shape = (n_inputs,n_neurous),stddev=stddev)  # 产生截断的正态分布\n",
    "#         W = tf.Variable(init,name ='weights')\n",
    "#         b = tf.Variable(tf.zeros([n_neurous]),name='biases') # 每个神经元 有一个偏置参数 默认为0\n",
    "#         z = tf.matmul(X,W)+b    # 该向量化实现将有效地计算输入的加权和 加上层中每个神经元的偏置，对于批次中的所有实例，仅需一次. \n",
    "#         if activation == 'relu':\n",
    "#             return tf.nn.relu(z)\n",
    "#         else:\n",
    "#             return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建神经网络\n",
    "\n",
    "第 一个隐藏层以 X 为输入。\t\n",
    "\n",
    "第二个将第一个隐藏层的输出作为其输入。\t\n",
    "\n",
    "最后，输出层将第二个隐藏层的输出作为其输入。\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with  tf.name_scope('dnn'):\n",
    "#     hidden1 = neuron_layer(X,n_hidden1,'hidden1',activation='relu')\n",
    "# hidden2 = neuron_layer(hidden1,n_hidden2,'hidden2',activation='relu')\n",
    "# logits = neuron_layer(hidden2,n_outputs,'outputs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请注意，为了清楚起见，我们再次使用名称范围。\t\n",
    "\n",
    "还要注意，logit\t是在通过\tsoftmax\t激活函 数之前神经网络的输出：为了优化，我们稍后将处理\tsoftmax\t计算。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用 tensorflow 内置的功能来创建标准的神经网络层\n",
    "\n",
    "通常不需要 像我们刚才那样定义你自己的\t`neuron_layer()`\t函数。\t\n",
    "\n",
    "例如，`TensorFlow` 的\t`fully_connected()`\t函数创建一个 __完全连接的层__ ，其中所有输入都连接到图层中的所有神经元。\t\n",
    "\n",
    "它使用正确的初始化策略来负责创建 __权重__ 和 __偏置变量__ ，并且 __默认情况__ 下使用\t`ReLU`\t激活函数（我们可以使用\t`activate_fn`\t参数来更改它）。\t\n",
    "#### 只需导入该功能，并使用以下代码替换\tdnn\t构建部分：\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.contrib.layers import fully_connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.name_scope('dnn'):\n",
    "#     hidden1 = fully_connected(X,n_hidden1,scope='hidden1')\n",
    "#     hidden2 = fully_connected(hidden1,n_hidden2,scope='hidden2')\n",
    "#     logits = fully_connected(hidden2,n_outputs,scope='outputs',activation_fn=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tensorflow.contrib`\t包包含许多有用的功能，但它是一个尚未分级成为主要\t`TensorFlow\tAPI` 一部分的实验代码的地方。因此,`full_connected()`\t函数（和任何其他 `contrib` 代码）可能会在将来更改或移动。\n",
    "\n",
    "### 所以：__使用 `dense()` 代替 `neuron_layer()`__\n",
    "\n",
    "__dense()__\t函数与\t__fully_connected()__\t函数几乎相同，除了一些细微的差别：\n",
    "\n",
    "* __scope__\t变为名称\n",
    "\n",
    "\n",
    "* __activation_fn__\t变为激活（同样\t\\_fn\t后缀从其他参数 （如\t__normalizer_fn__\t）中删除）\n",
    "\n",
    "\n",
    "* __weights_initializer__\t成为\t__kernel_initializer__\t等。\n",
    "\n",
    "\n",
    "* 默认激活现在是无，而不是\t__tf.nn.relu__\t。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-b1fcbf5dd1a7>:2: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From D:\\APYPLA~1\\venv_dir\\venv1\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('dnn'):\n",
    "    hidden1 = tf.layers.dense(X,n_hidden1,name='hidden1',activation=tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(hidden1,n_hidden2,name='hidden2',activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(hidden2,n_outputs,name='outputs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy,name='loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了 __神经网络__ 和 __损失函数__ 之后，接下来定义一个 __梯度下降优化器__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建模阶段的最后一个重要步骤是指定如何 __评估模型__。\t我们将简单地将精度用作我们的绩效指标\n",
    "\n",
    "首先，对于每个实例，通过检查最高\tlogit\t是否对应于目标类别来确定神经网络的预测是 否正确。\t为此，您可以使用\tin_top_k()\t函数。\t这返回一个充满布尔值的\t1D\t张量，因此我们 需要将这些布尔值转换为浮点数，然后计算平均值。\t这将给我们网络的整体准确性.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits,y,1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct,tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来 创建一个初始化所有变量的节点，我们还将创建一个\tSaver\t来 将我们训练有素的模型参数保存到磁盘中：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二、执行阶段"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 __tensorflow__ 自己提供的工具来加载 __mnist__ 数据集\n",
    "\n",
    "TensorFlow\t提供了自己的助手来获取数据，将其缩放（0\t到\t1\t之间），将 它洗牌，并提供一个简单的功能来一次加载一个小批量：\n",
    "\n",
    "\n",
    "值得注意的是：__read_data_sets__ 是用于读取压缩包的\n",
    "\n",
    "#### tensorflow.examples.tutorials 现在已经弃用了，推荐使用tensorflow.keras.datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "# from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mldata/train-images-idx3-ubyte.gz\n",
      "Extracting ./mldata/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mldata/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mldata/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('./mldata/')\n",
    "\n",
    "# (x_train,y_train),(x_test,y_test) = mnist.load_data('./mnist/mnist.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义 __迭代数__ 和 __小批量的大小__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 21\n",
    "batch_size = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 现在我们去训练模型:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy:  0.618 Test accuracy 0.6583\n",
      "1 Train accuracy:  0.8 Test accuracy 0.8137\n",
      "2 Train accuracy:  0.84 Test accuracy 0.8507\n",
      "3 Train accuracy:  0.874 Test accuracy 0.8696\n",
      "4 Train accuracy:  0.87 Test accuracy 0.88\n",
      "5 Train accuracy:  0.86 Test accuracy 0.8893\n",
      "6 Train accuracy:  0.882 Test accuracy 0.8962\n",
      "7 Train accuracy:  0.872 Test accuracy 0.9011\n",
      "8 Train accuracy:  0.908 Test accuracy 0.9043\n",
      "9 Train accuracy:  0.908 Test accuracy 0.9076\n",
      "10 Train accuracy:  0.89 Test accuracy 0.9104\n",
      "11 Train accuracy:  0.908 Test accuracy 0.9111\n",
      "12 Train accuracy:  0.89 Test accuracy 0.9136\n",
      "13 Train accuracy:  0.922 Test accuracy 0.9155\n",
      "14 Train accuracy:  0.882 Test accuracy 0.9164\n",
      "15 Train accuracy:  0.906 Test accuracy 0.9178\n",
      "16 Train accuracy:  0.906 Test accuracy 0.9199\n",
      "17 Train accuracy:  0.906 Test accuracy 0.9198\n",
      "18 Train accuracy:  0.894 Test accuracy 0.9202\n",
      "19 Train accuracy:  0.936 Test accuracy 0.9222\n",
      "20 Train accuracy:  0.914 Test accuracy 0.9231\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()    # 初始化之前定义的init\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            x_batch, y_batch  = mnist.train.next_batch(batch_size)       #其中的n代表返回多少个 训练数据集 和 对应的标签\n",
    "            sess.run(training_op,feed_dict={X:x_batch,y:y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X:x_batch,y:y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X:mnist.test.images,y:mnist.test.labels})\n",
    "        \n",
    "        print(epoch,'Train accuracy: ',acc_train,'Test accuracy',acc_test)\n",
    "    save_path = saver.save(sess,'./my_model_final.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，在每个时期结束时，代码评估最后一个小批量和完整训练集上的模型， 并打印出结果。\t最后，模型参数保存到磁盘。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  三、使用神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "[7 2 1 0 4 1 4 9 6 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 3 1 3 6 7 2 7\n",
      " 1 2 1 1 7 4 2 3 5 1 2 4 4]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,'./my_model_final.ckpt')\n",
    "    X_train = mnist.test.images[:50]\n",
    "    Z = logits.eval(feed_dict={X:X_train})\n",
    "    y_pred = np.argmax(Z,axis=1)\n",
    "    print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.7259476, -2.2674694,  6.025392 ,  2.3775709, -8.074056 ,\n",
       "        1.3455541,  3.325245 , -6.7976556,  0.4249972, -7.7564635],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6,\n",
       "       6, 5, 4, 0, 7, 4, 0, 1, 3, 1, 3, 4, 7, 2, 7, 1, 2, 1, 1, 7, 4, 2,\n",
       "       3, 5, 1, 2, 4, 4], dtype=uint8)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test.labels[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，代码从磁盘加载模型参数。\t然后加载一些您想要分类的新图像。\t记住应用与训练数据 相同的特征缩放（在这种情况下，将其从\t0\t缩放到\t1）。\t然后代码评估对数点节点。\t\n",
    "\n",
    "如果您 想知道所有估计的类概率，则需要将\tsoftmax()\t函数应用于对数，但如果您只想预测一个 类，则可以简单地选择具有最高\tlogit\t值的类（使用\targmax()\t函数做的伎俩）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 以上建模和训练流程：\n",
    "\n",
    "1、输入和目标创建占位符\n",
    "\n",
    "2、创建了一个构建神经元层的函数，我们用它来创建 DNN\n",
    "\n",
    "3、我们定义了损失函数\n",
    "\n",
    "4、我们创建了一 个优化器\n",
    "\n",
    "5、然后定义了性能指标。\t\n",
    "\n",
    "6、最后到执行阶段。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "venv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
