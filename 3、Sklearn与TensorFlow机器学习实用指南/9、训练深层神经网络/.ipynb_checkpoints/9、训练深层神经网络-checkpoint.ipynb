{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练深层神经网络\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度消失/爆炸问题\n",
    "\n",
    "反向传播算法的工作原理是从输出层到输入层，传播误差的梯度。 一旦该算法已经计算了网络中每个参数的损失函数的梯度，它就使用这些梯度来用梯度下降步骤来更新每个参数。\n",
    "\n",
    "不幸的是，梯度往往变得越来越小，随着算法进展到较低层。\t结果，梯度下降更新使得低层连接权重实际上保持不变，并且训练永远不会收敛到良好的解决方案。这被称为 __梯度消失问题__。\t\n",
    "\n",
    "在某些情况下，可能会发生相反的情况：梯度可能变得越来越大，许多层得到了非常大 的权重更新，算法发散。这是 __梯度爆炸的问题__。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  减缓这一问题的方法一：Xavier初始化和He初始化\n",
    "\n",
    "提出需要保持每一层的输入和输出的方差一致，并且需要在反向流动过某一层时，前后的方差也要一致。\n",
    "\n",
    "当输入连接的数量大致等于输出连接的数量时，可以得到更简单的等式：(就是第十章中用到的方差)\n",
    "\n",
    "</br>\n",
    "<div align=center><img width=\"400\" height=\"300\" src=\"./static/1.jpg\"/></div>\n",
    "\n",
    "该方法的折中方案公式为Xavier初始化。\n",
    "\n",
    "ReLU激活函数的初始化方法有时称为He初始化。\n",
    "\n",
    "#### He 初始化\n",
    "He\t初始化只考虑了扇入，而不是像\tXavier\t初始化那样扇入和扇出之间的平均值。\t\n",
    "\n",
    "这也 是\t`variance_scaling_initializer()`\t函数的默认值，但您可以通过设置参 数\t`mode\t=\"FAN_AVG\"`\t来更改它。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "# hidden1 = tf.layers.dense(X,n_hidden,activation=tf.nn.relu,kernel_initializer=he_init,name='hidden1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  减缓这一问题的方法二：更换激活函数\n",
    "\n",
    "一般来说\tELU\t>\tleaky\tReLU（及其变体）>\tReLU\t>\ttanh\t>\tsigmoid\n",
    "\n",
    "#### elu 激活函数\n",
    "\n",
    "tensorflow 中提供了 elu 函数用于建立神经网络："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden1 = tf.layers.dense(x,n_hidden,activation=tf.nn.elu,name='hidden1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### leaky\tReLU 激活函数\n",
    "\n",
    "tensorflow 中没有针对 leaky\tReLU 的函数，但是可以自己定义："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def leak_relu(z,name=None):\n",
    "#     return tf.maximum(0.01 * z,z,name = name)\n",
    "# hidden1 = tf.layers.dense(x,n_hidden1,activation=leak_relu,name='hidden1')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  减缓这一问题的方法三：批量标准化\n",
    "\n",
    "尽管使用\tHe初始化和\tELU（或任何\tReLU\t变体）可以显著减少训练开始阶段的梯度消失/爆炸问题，但不保证在训练期间问题不会回来。\n",
    "\n",
    "\n",
    "__该技术会在每一层激活函数之前在模型中介入一个操作，操作实现简单零中心化和归一化输入，之后再通过每层的两个新参数（一个缩放，一个移动）来控制缩放和移动的结果。这样的操作会让模型学会最佳规模和每层输入的平均值。__\n",
    "</br>\n",
    "<div align=center><img width=\"400\" height=\"300\" src=\"./static/2.jpg\"/></div>\n",
    "<div align=center><img width=\"660\" height=\"600\" src=\"./static/3.jpg\"/></div>\n",
    "\n",
    "在测试时，没有小批量计算经验均值和标准差，所以您只需使用整个训练集的均值和标准 差。\t这些通常在训练期间使用移动平均值进行有效计算。\t因此，总的来说，每个批次标准化 的层次都学习了四个参数：\t`γ（标度）`，\t`β（偏移）`，\t`μ（平均值）` 和\t`σ（标准差）`。\n",
    "\n",
    "#### 使用 BN 的优缺点\n",
    "\n",
    "再使用饱和激活函数的深度神经网络中，批量归一化取得了非常好的成绩，而且还会为降低后续的正则化的技术需求。\n",
    "\n",
    "但BN的使用确实也增加了模型的复杂度，降低了网络额速度。\n",
    "\n",
    "#### 使用\tTensorFlow\t实现批量标准化\n",
    "\n",
    "在 functools 模块中有一个工具partial()，可以用来\"冻结\"一个函数的参数，并返回\"冻结\"参数后的新函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.GraphKeys.UPDATE_OPS  \n",
    "\n",
    "`batch normalization` 的兩個重要的參數，`moving_mean` 和 `moving_var`,两个 `batch_normalization` 中更新 `mean` 和 `variance` 的操作，需要保证它们在train_op前完成。\n",
    "\n",
    "这两个操作是在 `tensorflow` 的内部实现中自动被加入 `tf.GraphKeys.UPDATE_OPS` 这个集合的，在 `tf.contrib.layers.batch_norm` 的参数中可以看到有一项`updates_collections` 的默认值即为 `tf.GraphKeys.UPDATE_OPS` ，而在 `tf.layers.batch_normalization` 中则是直接将两个更新操作放入了上述集合。\n",
    "\n",
    "如果不通过 `tf.get_collection` 来获取，`moving_mean` 和 `moving_var` 不会更新，一直都会是初始值。\n",
    "\n",
    "https://ithelp.ithome.com.tw/articles/10220410\n",
    "\n",
    "https://github.com/jason9075/ithome_tensorflow_series/blob/day17/17/update_op.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mldata/train-images-idx3-ubyte.gz\n",
      "Extracting ./mldata/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mldata/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mldata/t10k-labels-idx1-ubyte.gz\n",
      "0 Test accuracy: 0.8723\n",
      "1 Test accuracy: 0.8956\n",
      "2 Test accuracy: 0.9121\n",
      "3 Test accuracy: 0.9206\n",
      "4 Test accuracy: 0.9289\n",
      "5 Test accuracy: 0.9351\n",
      "6 Test accuracy: 0.9395\n",
      "7 Test accuracy: 0.9428\n",
      "8 Test accuracy: 0.9473\n",
      "9 Test accuracy: 0.949\n",
      "10 Test accuracy: 0.9521\n",
      "11 Test accuracy: 0.9518\n",
      "12 Test accuracy: 0.9559\n",
      "13 Test accuracy: 0.9584\n",
      "14 Test accuracy: 0.9585\n",
      "15 Test accuracy: 0.9609\n",
      "16 Test accuracy: 0.963\n",
      "17 Test accuracy: 0.9641\n",
      "18 Test accuracy: 0.9647\n",
      "19 Test accuracy: 0.9664\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "mnist = input_data.read_data_sets('./mldata/')\n",
    "batch_norm_momentum = 0.9\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape=(None,n_inputs),name = 'X')\n",
    "y = tf.placeholder(tf.int64,shape=None,name='y')\n",
    "\n",
    "training = tf.placeholder_with_default(False,shape=(),name = 'training') #给Batch\tnorm加一个placeholder\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "    \n",
    "    my_batch_norm_layer = partial(\n",
    "        tf.layers.batch_normalization,\n",
    "        training = training,\n",
    "        momentum = batch_norm_momentum\n",
    "    )\n",
    "\n",
    "    my_dense_layer = partial(\n",
    "        tf.layers.dense,\n",
    "        kernel_initializer = he_init\n",
    "    )\n",
    "    \n",
    "    hidden1 = my_dense_layer(X,n_hidden1,name='hidden1')\n",
    "    bn1 = tf.nn.elu(my_batch_norm_layer(hidden1))\n",
    "    \n",
    "    hidden2 = my_dense_layer(bn1,n_hidden2,name='hidden2')\n",
    "    bn2 = tf.nn.elu(my_batch_norm_layer(hidden2))\n",
    "    \n",
    "    logits_before_bn = my_dense_layer(bn2,n_outputs,name='outputs')\n",
    "    logits = my_batch_norm_layer(logits_before_bn)\n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y,logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy,name='loss')\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits,y,1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct,tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteraction in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch,y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run([training_op,extra_update_ops],feed_dict={training:True,X:X_batch,y:y_batch})\n",
    "        \n",
    "        accuracy_val = accuracy.eval(feed_dict={X:mnist.test.images,y:mnist.test.labels})\n",
    "        print(epoch, 'Test accuracy:', accuracy_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 或者将上面的 `train` 改写成："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.name_scope('train'):\n",
    "#     optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "#     extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "#     with tf.control_dependencies(extra_update_ops):\n",
    "#         training_op = optimizer.minimize(loss)\n",
    "\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     init.run()\n",
    "\n",
    "#     for epoch in range(n_epochs):\n",
    "#         for iteraction in range(mnist.train.num_examples // batch_size):\n",
    "#             X_batch,y_batch = mnist.train.next_batch(batch_size)\n",
    "#             sess.run(training_op,feed_dict={training:True,X:X_batch,y:y_batch})\n",
    "        \n",
    "#         accuracy_val = accuracy.eval(feed_dict={X:mnist.test.images,y:mnist.test.labels})\n",
    "#         print(epoch, 'Test accuracy:', accuracy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 这样，你只需要在训练过程中评估 training_op，TensorFlow 也会自动运行更新操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  减缓这一问题的方法四：梯度裁剪\n",
    "\n",
    "减少梯度爆炸问题的一种常用技术是在反向传播过程中简单地剪切梯度，使它们不超过某个 __阈值__\n",
    "\n",
    "一般 来说，人们更喜欢 __批量标准化__ ，但了解 __梯度裁剪__ 以及如何实现它仍然是有用的。\n",
    "\n",
    "在 TensorFlow 中，优化器的\t`minimize()`\t函数负责计算梯度并应用它们，所以您必须首先调用优化器的\t`compute_gradients()` 方法，然后使用\t`clip_by_value()` 函数创建一个 `裁剪梯度` 的 操作，最后创建一个操作来使用优化器的 `apply_gradients()`\t方法应用裁剪梯度：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold = 1.0\n",
    "\n",
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "# grad_and_var = optimizer.compute_gradients(loss)\n",
    "# capped_gvs =  [(tf.clip_by_value(grad, -threshold, threshold), var)for grad, var in grads_and_vars] \n",
    "# training_op = optimizer.apply_gradients(capped_gvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "像往常一样，您将在每个训练阶段运行这个\ttraining_op\t。\t它将计算梯度，将它们裁剪到 -1.0\t和\t1.0\t之间，并应用它们。\t\tthrehold\t是您可以调整的超参数。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 复用预训练层\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从零开始训练一个非常大的\tDNN\t通常不是一个好主意，相反，__您应该总是尝试找到一个现有 的神经网络来完成与您正在尝试解决的任务类似的任务，然后复用这个网络的较低层：这就 是所谓的迁移学习__ 。这不仅会大大加快训练速度，还将需要更少的训练数据。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 比如现在有这样一个 tensorflow 模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_val: 0.8887\n",
      "accuracy_val: 0.9305\n",
      "accuracy_val: 0.9426\n",
      "accuracy_val: 0.9489\n",
      "accuracy_val: 0.9585\n",
      "accuracy_val: 0.9593\n",
      "accuracy_val: 0.9649\n",
      "accuracy_val: 0.9676\n",
      "accuracy_val: 0.9676\n",
      "accuracy_val: 0.9691\n",
      "accuracy_val: 0.9706\n",
      "accuracy_val: 0.9735\n",
      "accuracy_val: 0.9714\n",
      "accuracy_val: 0.9721\n",
      "accuracy_val: 0.9721\n",
      "accuracy_val: 0.9716\n",
      "accuracy_val: 0.9732\n",
      "accuracy_val: 0.974\n",
      "accuracy_val: 0.9716\n",
      "accuracy_val: 0.9742\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 50\n",
    "n_hidden5 = 50\n",
    "\n",
    "n_outputs = 10\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape=(None,n_inputs),name='X')\n",
    "y = tf.placeholder(tf.int32,shape=None,name='y')\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    hidden1 = tf.layers.dense(X,n_hidden1,activation=tf.nn.relu,name='hidden1')\n",
    "    hidden2 = tf.layers.dense(hidden1,n_hidden2,activation=tf.nn.relu,name='hidden2')\n",
    "    hidden3 = tf.layers.dense(hidden2,n_hidden3,activation=tf.nn.relu,name='hidden3')\n",
    "    hidden4 = tf.layers.dense(hidden3,n_hidden4,activation=tf.nn.relu,name='hidden4')\n",
    "    hidden5 = tf.layers.dense(hidden4,n_hidden5,activation=tf.nn.relu,name='hidden5')\n",
    "\n",
    "    logits = tf.layers.dense(hidden5,n_outputs,name='outputs')\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy,name='loss')\n",
    "    \n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits,y,1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct,tf.float32),name='accuracy')\n",
    "\n",
    "learning_rate = 0.01\n",
    "threshold = 1.0\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "capped_gvs =  [(tf.clip_by_value(grad, -threshold, threshold), var)for grad, var in grads_and_vars] \n",
    "training_op = optimizer.apply_gradients(capped_gvs)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    init.run()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        for iteraction in range(mnist.train.num_examples // batch_size):\n",
    "            x_batch,y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run([training_op],feed_dict={X:x_batch,y:y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X:mnist.test.images,y:mnist.test.labels})\n",
    "        print('accuracy_val:',accuracy_val)\n",
    "    save_path = saver.save(sess,'./my_new_model_final.ckep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们要复用之前模型的前几层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300 # reused\n",
    "n_hidden2 = 50  # reused\n",
    "n_hidden3 = 50  # reused\n",
    "\n",
    "n_hidden4 = 20  # new!\n",
    "n_outputs = 10  # new!\n",
    "\n",
    "X = tf.placeholder(tf.float32,shape=(None,n_inputs),name='X')\n",
    "y = tf.placeholder(tf.int32,shape=None,name='y')\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    hidden1 = tf.layers.dense(X,n_hidden1,activation=tf.nn.relu,name='hidden1')\n",
    "    hidden2 = tf.layers.dense(hidden1,n_hidden2,activation=tf.nn.relu,name='hidden2')\n",
    "    hidden3 = tf.layers.dense(hidden2,n_hidden3,activation=tf.nn.relu,name='hidden3')\n",
    "    hidden4 = tf.layers.dense(hidden3,n_hidden4,activation=tf.nn.relu,name='hidden4')\n",
    "\n",
    "    logits = tf.layers.dense(hidden4,n_outputs,name='outputs')\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy,name='loss')\n",
    "\n",
    "    \n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits,y,1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct,tf.float32),name='accuracy')\n",
    "\n",
    "[...] # build new model with the same definition as before for hidden layers 1-3\n",
    "\n",
    "reuse_vars = tf.get_collection(tf.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "venv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
