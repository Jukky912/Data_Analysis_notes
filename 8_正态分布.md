# 正态分布

  ## 什么叫正态分布：
    
   什么是正态分布呢？正态分布也叫作常态分布，就是正常的状态下，呈现的分布情况。
  
  举个例子：
  
  比如你可能会问班里的考试成绩是怎样的？这里其实指的是大部分同学的成绩如何。以下图为例，在正态分布中，大部分人的成绩会集中在中间的区域，少部分人处于两头的位置。正态分布的另一个好处就是，如果你知道了自己的成绩，和整体的正态分布情况，就可以知道自己的成绩在全班中的位置。

![Image text](https://raw.githubusercontent.com/OneStepAndTwoSteps/data_mining_analysis/master/static/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83.jpg)

另一个典型的例子就是，美国 SAT 考试成绩也符合正态分布。而且美国本科的申请，需要中国高中生的 GPA 在 80 分以上（百分制的成绩），背后的理由也是默认考试成绩属于正态分布的情况。


为了让成绩符合 __正态分布__ ，出题老师是怎么做的呢？他们通常可以把考题分成三类：

    第一类：基础题，占总分 70%，基本上属于送分题；

    第二类：灵活题，基础范围内 + 一定的灵活性，占 20%；

    第三类：难题，涉及知识面较广的难题，占 10%；

  那么，你想下，如果一个出题老师没有按照上面的标准来出题，而是将第三类难题比重占到了 70%，也就是我们说的“超纲”，结果会是怎样呢？

你会发现，大部分人成绩都“不及格”，最后在大家激烈的讨论声中，老师会将考试成绩做 __规范化处理__ ，从而让成绩 __满足正态分布的情况__ 。因为只有这样， __成绩才更具有比较性。所以正态分布的成绩，不仅可以让你了解全班整体的情况，还能了解每个人的成绩在全班中的位置。
 
## 数据的变换
  
  举个例子，假设A同学考了80分，B同学也考了80分，但是A同学的试卷是100分制，B同学的试卷是1000分制，如果我们把从这两个渠道收集上来的数据进行集成、挖掘，就算使用效率再高的算法，结果也不是正确的。因为这两个渠道的 __分数代表的含义__ 完全不同。
  
  __数据变换__ 就是让不同渠道的数据统一到一个目标数据库里，同时保证含义一致。
  
  __所以数据的准备至关重要__
    
    在数据变换前，我们需要先对字段进行筛选，然后对数据进行探索和相关性分析，接着是选择算法模型（这里暂时不需要进行模型计算），然后针对算法模型对数据的需求进行数据变换，从而完成数据挖掘前的准备工作。
  
  __数据挖掘前的准备流程：__
    
    字段过滤 --> 数据探索 --> 相关性分析 --> 建模筛选 --> 数据变换
  
  从整个流程中可以看出，数据变换是数据准备的重要环节，它__通过数据平滑、数据聚集、数据概化和规范化等方式__ 将数据转换成适用于数据挖掘的形式。
  
  __常见的数据变换的方法：__
  
    数据平滑：去除数据中的噪声，将连续数据离散化。这里可以采用分箱、聚类和回归的方式进行数据平滑，我会在后面给你讲解聚类和回归这两个算法；

    数据聚集：对数据进行汇总，在 SQL 中有一些聚集函数可以供我们操作，比如 Max() 反馈某个字段的数值最大值，Sum() 返回某个字段的数值总和；

    数据概化：将数据由较低的概念抽象成为较高的概念，减少数据复杂度，即用更高的概念替代更低的概念。比如说上海、杭州、深圳、北京可以概化为中国。

    数据规范化：使属性数据按比例缩放，这样就将原来的数值映射到一个新的特定区域中。常用的方法有最小—最大规范化、Z—score 规范化、按小数定标规范化等，我会在后面给你讲到这些方法的使用；

    属性构造：构造出新的属性并添加到属性集中。这里会用到特征工程的知识，因为通过属性与属性的连接构造新的属性，其实就是特征工程。比如说，数据表中统计每个人的英语、语文和数学成绩，你可以构造一个“总和”这个属性，来作为新属性。这样“总和”这个属性就可以用到后续的数据挖掘计算中。

  __数据进行规范处理的三种方法：__
    
   __1. Min-max 规范化__

    Min-max 规范化方法是将原始数据变换到 [0,1] 的空间中。
    公式：新数值 =（原数值 - 极小值）/（极大值 - 极小值）。
  
   __2. Z-Score 规范化__
  
  假设 A 与 B 的考试成绩都为 80 分，A 的考卷满分是 100 分（及格 60 分），B 的考卷满分是 500 分（及格 300 分）。虽然两个人都考了 80 分，但是 A 的 80 分与 B 的 80 分代表完全不同的含义。
  那么如何用相同的标准来比较 A 与 B 的成绩呢？Z-Score 就是用来可以解决这一问题的。
  
    公式：新数值 =（原数值 - 均值）/ 标准差。
  
  假设 A 所在的班级平均分为 80，标准差为 10。B 所在的班级平均分为 400，标准差为 100。那么 A 的新数值 =(80-80)/10=0，B 的新数值 =(80-400)/100=-3.2。
  
  那么在 Z-Score 标准下，A 的成绩会比 B 的成绩好。
  
  __Z-Score 的优点：__
      
      算法简单，不受数据量级影响，结果易于比较。
  
   __Z-Score 的不足：__
  
      它需要数据整体的平均值和方差，而且结果没有实际意义，只是用于比较。
  
  
  __3. 小数定标规范化__
  
      小数定标规范化就是通过移动小数点的位置来进行规范化。小数点移动多少位取决于属性 A 的取值中的最大绝对值。
  
  举个例子:
  
    比如属性 A 的取值范围是 -999 到 88，那么最大绝对值为 999，小数点就会移动 3 位，即新数值 = 原数值 /1000。那么 A 的取值范围就被规范化为 -0.999 到 0.088。
  
  
  ## Python 的 SciKit-Learn 库使用
  
   SciKit-Learn 是 Python 的重要机器学习库，它帮我们封装了大量的机器学习算法，比如分类、聚类、回归、降维等。此外，它还包括了数据变换模块。
  
  __1. Min-max 规范化__
  
  我们可以让原始数据投射到指定的空间 [min, max]，在 SciKit-Learn 里有个函数 MinMaxScaler 是专门做这个的，它允许我们给定一个最大值与最小值，然后将原数据投射到 [min, max] 中。默认情况下 [min,max] 是 [0,1]，也就是把原始数据投放到 [0,1] 范围内。
  
  例子：
  
      # coding:utf-8
      from sklearn import preprocessing
      import numpy as np
      # 初始化数据，每一行表示一个样本，每一列表示一个特征
      x = np.array([[ 0., -3.,  1.],
                    [ 3.,  1.,  2.],
                    [ 0.,  1., -1.]])
      # 将数据进行 [0,1] 规范化
      min_max_scaler = preprocessing.MinMaxScaler()
      minmax_x = min_max_scaler.fit_transform(x)
      print minmax_x

 运行结果：
 
     [[0.         0.         0.66666667]
     [1.         1.         1.        ]
     [0.         1.         0.        ]]

 公式：新数值 =（原数值 - 极小值）/（极大值 - 极小值）。
    
      例：新数值=(0-0)/3
  
  __2. Z-Score 规范化__
  
  在 SciKit-Learn 库中使用 preprocessing.scale() 函数，可以直接将给定数据进行 Z-Score 规范化。
  
  例子：
    
    from sklearn import preprocessing
    import numpy as np
    # 初始化数据
    x = np.array([[ 0., -3.,  1.],
                  [ 3.,  1.,  2.],
                  [ 0.,  1., -1.]])
    # 将数据进行 Z-Score 规范化
    scaled_x = preprocessing.scale(x)
    print scaled_x

  
  运行结果：
  
      [[-0.70710678 -1.41421356  0.26726124]
       [ 1.41421356  0.70710678  1.06904497]
       [-0.70710678  0.70710678 -1.33630621]]

  这个结果实际上就是将每行每列的值减去了平均值，再除以方差的结果。
  
  __注意：
  
    scale(X, axis=0, with_mean=True, with_std=True, copy=True)
    轴用于计算平均值和标准偏差。 如果为0，独立标准化每个功能，否则（如果1）标准化每个样本。
  
  公式：新数值 =（原数值 - 均值）/ 标准差。
    
    使用np.std(x，axis=0) 得出标准差为 [1.41421356 1.88561808 1.24721913] 
    
    使用np.std(x，axis=0) 得出均值为 [ 1.         -0.33333333  0.66666667]
    
    拿3举例： 新值=(3-1)/1.414=1.41421356
    
    拿-3举例： 新值=(-3--0.33333333)/1.88561808=-1.41421356
    
    不加axis默认计算全部的 标准差和均值 分别为1.640535895581489和0.4444444444444444 与本次计算无关。
    
    
    
 __3. 小数定标规范化   __
 
    我们需要用 NumPy 库来计算小数点的位数。NumPy 库我们之前提到过。
    
   例子：
      
        # coding:utf-8
        from sklearn import preprocessing
        import numpy as np
        # 初始化数据
        x = np.array([[ 0., -3.,  1.],
                      [ 3.,  1.,  2.],
                      [ 0.,  1., -1.]])
        # 小数定标规范化
        j = np.ceil(np.log10(np.max(abs(x))))
        scaled_x = x/(10**j)
        print scaled_x

   运行结果：
    
        [[ 0.  -0.3  0.1]
         [ 0.3  0.1  0.2]
         [ 0.   0.1 -0.1]]

## 总结：
   
   在数据变换中， __重点是如何将数值进行规范化__ ，有三种常用的规范方法，分别是 Min-Max 规范化、Z-Score 规范化、小数定标规范化。其中 __Z-Score 规范化可以直接将数据转化为正态分布的情况__ ，当然不是所有自然界的数据都需要正态分布，我们也可以根据实际的情况进行设计，比如取对数 log，或者神经网络里采用的激励函数等。
    
    
    

