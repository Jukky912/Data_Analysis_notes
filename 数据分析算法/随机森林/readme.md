## 集成算法-随机森林

### 集成算法模型：

__Bagging:__ __并行。__ 训练多个分类器取平均,如随机森林中我们并行运行多个随机数，如果是回归任务我们可以取决策树们的平均值，如果是分类问题我们可以取分类结果中的众数(比如分类结果 0 和 1 ，如果决策出的结果0较多，那么最终分类结果取0) __典型的算法就是随机森林__，可以帮助我们提高泛化能力。

__Boosting:__ __串行。__ 从弱学习器开始加强，通过加权来进行训练，__常见的Boosting模型算法有Adaboost，Xgboost算法__。

举个例子：当我们去银行贷款1000元，但是第一颗树模型(A)预测结果为贷款950元，但是我们的真实值是贷款了100，我们的误差为1000-950=50，接下来我们就需要减少这个误差(50),在我们第二棵树模型(B)就需要在这个50上再进行预测，假设预测结果为30，假设我们就只有三颗树，那么第三颗树(C) 就不再看第B颗树的结果了，而是结合A和B的结果(1000-950-30=20),C树预测的残差值就是20，这个时候如果C数预测的是18，这个时候根据下面的公式，我们将我们的所有树的结果相加我们的预测值就是998，这个时候我们的效果就很好了。按这个例子来讲的话boosting就是算出A树然后在A树的基础上计算出残差，然后再构造B树，然后C树。串行。

__Stacking:__ 堆叠算法，可以堆叠各种各样的分类器（KNN,SVM,RF等等），堆叠算法分阶段：第一阶段得出各自结果，第二阶段再用前一阶段结果训练。

举个例子：__第一阶段：__ 现在我们分别构建了RF、LR、KNN、SVM算法分类模型，分类器之间的分类结果可能不同，假如我们现在的分类是二分类，现在分类第一个样本，可能 RF分类的结果为1、LR分类的结果为1、KNN分类的结果为1、SVM分类的结果为0，分类第二个样本，可能 RF分类的结果为0、LR分类的结果为1、KNN分类的结果为1、SVM分类的结果为0，分类第三个样本，可能 RF分类的结果为0、LR分类的结果为0、KNN分类的结果为0、SVM分类的结果为0。接下来开始 __第二个阶段：__ 第二个阶段会将第一个阶段的分类结果输入到一个模型(如LR模型)中，然后输出最终的分类结果。

__简而言之:__ 第一个阶段：我们制造多个模型(如分类器)，然后进行特征的输入，得到各个模型的预测结果。第二个阶段，我们将第一个阶段的所有预测结果作为我们这个阶段的输入，输入到我们的一个模型中，最终输出我们的最终结果。



### 随机森林

我们先按照随机森林这个名称进行讲解：

随机：1、数据采样随机(如随机抽取60%的样本进行训练) 2、特征选择随机(在样本中随机抽取60%的特征进行训练) (每棵树数据量都是一致的) 因为二重随机，所以我们构造出的树基本都是不同的。

森林：很多颗决策树组成的森林。


__随机森林优势:__ 

1、它能够处理很高维度（feature很多）的数据，并且不用做特征选择

2、在训练完后，它能够给出哪些feature比较重要

如何比较哪个feature比较重要呢？

    



3、容易做成并行化方法，速度比较快

4、可以进行可视化展示，便于分析



### 注意 

1、在随机森林中并不是决策树越多越好，实际上基本超过一定数量准确率就会在差不多的基础上上下浮动



